{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import script\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "MODEL_WEIGHTS_PATH = ROOT_DIR +\"/mask_rcnn_coco.h5\"\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve, multilabel_confusion_matrix, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = script.CustomConfig()\n",
    "BEAGLE_DIR = ROOT_DIR+\"/fine-tune-MaskRcnn/glitch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    BACKBONE = 'resnet101'\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target device\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = beagle.CustomDataset()\n",
    "dataset.load_custom(GLITCH_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"../logs/WEIGHTFILE.h5\"\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "y_true2 = []\n",
    "y_pred2 = []\n",
    "\n",
    "y_true = []\n",
    "y_scores = []\n",
    "counts = [0, 0]\n",
    "\n",
    "elapsed = 0\n",
    "\n",
    "\n",
    "# image_ids = np.random.choice(dataset.image_ids, 80)\n",
    "for image_id in dataset.image_ids:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "    results = model.detect([image], verbose=1)\n",
    "    ax = get_ax(1)\n",
    "    r = results[0]\n",
    "    \n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                    dataset.class_names, r['scores'], ax=ax,\n",
    "                                    title=\"Predictions\")\n",
    "    end = time.time()\n",
    "    elapsed = elapsed + (end-start)\n",
    "    \n",
    "    totalimg = np.zeros((1024, 1024))\n",
    "    for i in range(r['masks'].shape[-1]):\n",
    "        mask = r['masks'][:, :, i]\n",
    "        totalimg = np.add(totalimg, mask)\n",
    "        image[mask] = 255\n",
    "        image[~mask] = 0\n",
    "        unique, counts = np.unique(image, return_counts=True)\n",
    "        \n",
    "        if not np.all((gt_mask == 0)): #if not completely normal\n",
    "            groundtruth = np.reshape(gt_mask, (-1, gt_mask.shape[-1])).astype(np.float32)\n",
    "        #         predicted = float(np.count_nonzero(totalimg))\n",
    "\n",
    "            print(np.shape(gt_mask))\n",
    "            print(np.shape(totalimg))\n",
    "\n",
    "            gtruth = gt_mask[:,:,0]\n",
    "\n",
    "            for i in range(0, gt_mask.shape[-1]-1):\n",
    "                print(i)\n",
    "                out = np.nonzero((totalimg!=0) & (gt_mask[:,:,i]!=0))\n",
    "\n",
    "            if len(counts) == 1:\n",
    "                y_scores.append(0)\n",
    "            else:\n",
    "                y_scores.append(2 * np.count_nonzero(out) / (float(np.count_nonzero(totalimg)) + groundtruth.sum()))            \n",
    "    \n",
    "    if (len(gt_class_id) == 0):\n",
    "        y_true.append(0)\n",
    "    else:\n",
    "        y_true.append(1)\n",
    "\n",
    "    predictionVector = np.zeros(4)\n",
    "    \n",
    "    for i in range(len(r['class_ids'])):\n",
    "        for c in range(1,4): # c = 1:missing 2:low_res 3:stretched\n",
    "            if r['class_ids'][i] == c and r['scores'][i] > predictionVector[c]:\n",
    "                predictionVector[c] = r['scores'][i]\n",
    "    if np.array_equal(predictionVector, np.zeros(4)):\n",
    "        predictionVector[0] = 1.0\n",
    "\n",
    "    groundtruthVector =  np.zeros(4)\n",
    "    \n",
    "    for p in gt_class_id:\n",
    "        groundtruthVector[p] = 1.0\n",
    "    if np.array_equal(groundtruthVector, np.zeros(4)):\n",
    "        groundtruthVector[0] = 1.0\n",
    "\n",
    "    print(predictionVector, groundtruthVector)\n",
    "    \n",
    "    if predictionVector[0] == groundtruthVector[0]:\n",
    "        if groundtruthVector[0] == 0:\n",
    "            TN += 1\n",
    "        else:\n",
    "            TP += 1\n",
    "    else:\n",
    "        if groundtruthVector[0] == 0:\n",
    "            FP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    \n",
    "    y_true2.append(groundtruthVector)\n",
    "    y_pred2.append(np.round(predictionVector))\n",
    "\n",
    "\n",
    "print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(TP, TN, FP, FN) \n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(precision, recall)\n",
    "print(f1score) \n",
    "\n",
    "y_pred2 = np.round(y_pred2)\n",
    "\n",
    "# TN FN\n",
    "# FP TP\n",
    "\n",
    "cm = multilabel_confusion_matrix(y_true2, y_pred2)\n",
    "print(cm)\n",
    "print(classification_report(y_true2,y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_scores)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "\n",
    "plt.figure()\n",
    "plt.axis('equal')\n",
    "plt.plot(recall, precision, lw=2, color='navy')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.title('Precision-Recall AUC={0:0.2f}'.format(average_precision_score(\n",
    "        y_true, y_scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
